{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What do people want to know about coronavirus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Webscraping\n",
    "- Get frequently asked question from CDC webpage\n",
    "- Store question answer pairs in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDC questions\n",
    "\n",
    "urls = [\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/hcp/faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/prevent-getting-sick/cloth-face-cover-faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/tribal/faq-burial-practice.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/schools-childcare/schools-faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/colleges-universities/faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/large-events/event-planners-and-attendees-faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/retirement/faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/correction-detention/faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/hcp/faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/travelers/faqs.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/covid-data/faq-surveillance.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/general-business-faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/wildland-firefighters-faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/law-enforcement-agencies-faq.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/community/homeless-shelters/faqs.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/php/water.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/lab/biosafety-faqs.html\",\n",
    "    \"https://www.cdc.gov/coronavirus/2019-ncov/lab/testing-laboratories.html\"   \n",
    "]\n",
    "questions = []\n",
    "results = {}\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "    for content in soup.find_all(class_=\"card bar\"):\n",
    "        question = content.find(\"span\").text\n",
    "        questions.append(question)\n",
    "        answer = content.find(\"p\").text\n",
    "\n",
    "        p = content.find_all(\"li\")\n",
    "        if p != None:\n",
    "            for i in range(0, len(p)):\n",
    "                answer += \" \" + p[i].text \n",
    "                if (i != len(p) - 1):\n",
    "                    answer += \",\"\n",
    "                    \n",
    "        results[question] = answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHO questions\n",
    "\n",
    "questionsHubUrl = \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/question-and-answers-hub\"\n",
    "page = requests.get(questionsHubUrl).text\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "urls = [str(\"http://www.who.int\" + link['href']) for link in soup.find_all(class_=\"sf-list-vertical__item\") if link.has_attr('href')]\n",
    "\n",
    "for url in urls:\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    for content in soup.find_all(class_=\"sf-accordion__panel\"):\n",
    "        question = content.find('a').text.strip()\n",
    "        questions.append(question)\n",
    "        answer = [c.text.strip() for c in content.find_all('p')]\n",
    "        answer = reduce(lambda x,y: x + ' ' + y, answer)\n",
    "        results[question] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FDA questions\n",
    "\n",
    "url = \"https://www.fda.gov/medical-devices/emergency-situations-medical-devices/faqs-testing-sars-cov-2\"\n",
    "page = requests.get(url).text\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "qTemp = []\n",
    "aTemp = []\n",
    "\n",
    "for content in soup.find_all(class_=\"panel-title\"):\n",
    "    question = content.find('a')\n",
    "    if question != None:\n",
    "        qTemp.append(question.text[3:].strip().split('?')[0])\n",
    "\n",
    "for content in soup.find_all(class_=\"panel-body\"):\n",
    "    answer = [c.text.strip() for c in content.find_all('p')]\n",
    "    answer = reduce(lambda x,y: x + ' ' + y, answer)\n",
    "    if answer != None and answer[0] == 'A':\n",
    "        aTemp.append(answer[3:])\n",
    "\n",
    "for i in range(len(qTemp)):\n",
    "    questions.append(qTemp[i])\n",
    "    results[qTemp[i]] = aTemp[i] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615\n"
     ]
    }
   ],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords, correcting typos\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean1(sentence):\n",
    "    sentence = TextBlob(sentence).correct()\n",
    "    words = list(sentence.words)\n",
    "    words = [w for w in words if w.isalpha()] #Remove punctuation\n",
    "    return [w for w in words if not w in stopWords] #Return nonstop word sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize and remove duplicates\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean2(words):\n",
    "    return set([Word(w).lemmatize() for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Compare simplified question to question in our dataset, store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# with open(\"simplified_qs.txt\", \"wb\") as file:\n",
    "#     pickle.dump([clean2(clean1(q)) for q in questions], file)\n",
    "    \n",
    "simplifiedQs = pickle.load(open(\"simplified_qs.txt\", \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize words\n",
    "import nltk.tokenize\n",
    "\n",
    "def calculateComparisonScore(a, b):\n",
    "    score = 0\n",
    "    commonElementWithTags = nltk.pos_tag(set(a) & set(b))\n",
    "    for w in commonElementWithTags:\n",
    "        if w[1] == 'NNP':\n",
    "            score += 3\n",
    "        elif w[1] == \"NN\":\n",
    "            score += 2\n",
    "        elif w[1] == 'VB':\n",
    "            score += 1\n",
    "        else:\n",
    "            score += 0.1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to chunk noun phrases\n",
    "import numpy as np\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "def findChunk(question, regex):\n",
    "    tokenized = nltk.sent_tokenize(question)\n",
    "    words = nltk.word_tokenize(tokenized[0])\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    cp = nltk.RegexpParser(regex)\n",
    "    chunks = cp.parse(tagged)\n",
    "    chunks = [chunk for chunk in chunks if chunk[0][1] == 'NNP']\n",
    "    for chunk in chunks:\n",
    "        val = \"\"\n",
    "        for value in chunk:\n",
    "            val += str(value[0]) + \" \"\n",
    "        print(val)\n",
    "        \n",
    "findChunk(\"What is the John Stamos compared to the CDC\", \"NNP: {<NNP>+}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry but we don't seem to have an answer for your specific question\n"
     ]
    }
   ],
   "source": [
    "inputQuestion = \"corona\"\n",
    "\n",
    "cleaned = clean2(clean1(inputQuestion))\n",
    "\n",
    "bestMatch = 0\n",
    "best = 0\n",
    "\n",
    "\n",
    "for i in range(0, len(simplifiedQs)):\n",
    "    commonAmount = calculateComparisonScore(simplifiedQs[i], cleaned)\n",
    "    if commonAmount > best:\n",
    "        best = commonAmount\n",
    "        bestMatch = i\n",
    "        \n",
    "if best == 0:\n",
    "    print(\"Sorry but we don't seem to have an answer for your specific question\")\n",
    "else:\n",
    "    print(questions[bestMatch] + \"\\n\")\n",
    "    print(results[questions[bestMatch]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
